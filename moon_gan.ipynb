{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56919b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "batch_size = 64\n",
    "latent_dim = 60  # Dimension of the latent space\n",
    "channels = 1  # Number of channels in the input data and generated images\n",
    "lr = 0.00008  #0.0001\n",
    "b1, b2 = 0.7, 0.999  # Beta parameters for Adam optimizer\n",
    "n_epochs = 80\n",
    "#n_ch = 128  # Number of channels in initial convolution layers\n",
    "#sample_interval = 100  # Save a generated image every sample_interval number of batches\n",
    "d_conv_channels = 4  # Number of channels in the Discriminators convlution operation\n",
    "board_size = (18, 11)\n",
    "\n",
    "drop_top_n = 4  # Exclude the problems with the top this many grades\n",
    "drop_bottom_n = 1  # Exclude the problems with the bottom this many grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91181d0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grade_dict = {'6A': 1, '6A+': 2, '6B': 3, '6B+': 4, '6C': 5, '6C+': 6, '7A': 7, '7A+': 8,\n",
    "              '7B': 9, '7B+': 10, '7C': 11, '7C+': 12, '8A': 13, '8A+': 14, '8B': 15, '8B+': 16}\n",
    "\n",
    "# Convert grade n from ints to 1D tensors with ones in the first n positions and zeros elsewhere (one-hot encoding).\n",
    "# Ex. 4 -> [1,1,1,1,0,0,0,0,0,...]\n",
    "# This allows for ordinal regression in the loss function\n",
    "# i.e. loss is greater for predictions further from the true grade\n",
    "grade_dict_vec = {}\n",
    "for grade in grade_dict:\n",
    "    vec = t.zeros(len(grade_dict) - drop_bottom_n - drop_top_n)\n",
    "    vec[0:grade_dict[grade]] = 1\n",
    "    grade_dict_vec[grade] = vec\n",
    "\n",
    "num_grades = len(list(grade_dict_vec.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc17d75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "problem_path = 'problems.pkl'\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        with open('problems1.pkl', 'rb') as f:\n",
    "            data1 = pickle.load(f)\n",
    "\n",
    "        with open('problems2.pkl', 'rb') as f:\n",
    "            data2 = pickle.load(f)\n",
    "\n",
    "        data = {**data1, **data2}\n",
    "\n",
    "        self.names = data.keys()\n",
    "        self.grades, self.start_holds, self.mid_holds, self.end_holds, self.all_holds = [], [], [], [], []\n",
    "        for name in self.names:\n",
    "            problem = data[name]\n",
    "            if problem[0] in grade_dict.keys():\n",
    "                if grade_dict[problem[0]] in (max(grade_dict.values()) -\n",
    "                                              np.array(range(drop_top_n))):\n",
    "                    continue\n",
    "                if grade_dict[problem[0]] in range(1, drop_bottom_n + 1):\n",
    "                    continue\n",
    "                self.grades.append(problem[0])\n",
    "                self.start_holds.append(problem[1])\n",
    "                self.mid_holds.append(problem[2])\n",
    "                self.end_holds.append(problem[3])\n",
    "                self.all_holds.append(problem[4])\n",
    "\n",
    "        self.all_holds_split_channels = t.Tensor(\n",
    "            [[self.start_holds[i], self.mid_holds[i], self.end_holds[i]]\n",
    "             for i in range(len(self.start_holds))])\n",
    "\n",
    "        self.start_holds = t.Tensor(self.start_holds)\n",
    "        self.mid_holds = t.Tensor(self.mid_holds)\n",
    "        self.end_holds = t.Tensor(self.end_holds)\n",
    "        self.all_holds = t.Tensor(self.all_holds)\n",
    "\n",
    "        self.all_holds_neg_ends = self.mid_holds - self.start_holds - self.end_holds\n",
    "\n",
    "        self.grades_numeric = [grade_dict[grade] for grade in self.grades]\n",
    "        self.grades = [grade_dict_vec[grade] for grade in self.grades]\n",
    "\n",
    "        self.len = len(self.grades)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.all_holds[index], self.grades[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9c3203",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # Takes as input a random tensor of size (batch_size, latent_dim) and sampled grades\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        fc1_size = 99\n",
    "        fc2_size = np.prod(board_size)\n",
    "        conv_channels = 16\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(num_grades, latent_dim), nn.ReLU())\n",
    "\n",
    "        self.lb1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(2*latent_dim),\n",
    "            nn.Linear(2*latent_dim, fc1_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lb2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(fc1_size),\n",
    "            nn.Linear(fc1_size, fc2_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.cb1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=conv_channels,\n",
    "                      kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.5)\n",
    "        )\n",
    "        self.cb2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=conv_channels,\n",
    "                      out_channels=1, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        z, grade = z\n",
    "        emb = self.fc1(grade)\n",
    "\n",
    "        # Concatenate grade and noise (z)\n",
    "        gen = t.cat([z, emb], 1)\n",
    "\n",
    "        # Run the linear blocks\n",
    "        gen = self.lb1(gen)\n",
    "        gen = self.lb2(gen)\n",
    "\n",
    "        # [batch_size, 18*11] -> [batch_size, 1, 18, 11]\n",
    "        gen = gen.view(-1, 1, 18, 11)\n",
    "\n",
    "        # Run the convolutional blocks\n",
    "        gen = self.cb1(gen)\n",
    "        return self.cb2(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b46eb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    # Takes as input a data image or image generated by Generator\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Conv2d(in_channels=1, out_channels=d_conv_channels, kernel_size=(\n",
    "            11, 7), padding=(5, 3), stride=1)  # Convolution with 4 filters of size 11x7\n",
    "        # Convolution with 1 filter of size 1 to feed info directly to next layer.  Bias set to false so that non-holds are set to zero\n",
    "        self.bypass_layer = nn.Conv2d(\n",
    "            in_channels=1, out_channels=1, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        # First fc layer with 5x18x11 neurons -> 50 neurons\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear((d_conv_channels + 1) * np.prod(board_size), 50), nn.Sigmoid())\n",
    "        # Dropout layer to reduce overtraining\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc_grade = nn.Sequential(\n",
    "            nn.Linear(50, num_grades), nn.Sigmoid())  # Second fc layer\n",
    "        self.fc_rf = nn.Sequential(nn.Linear(50, 1), nn.Sigmoid())\n",
    "\n",
    "#         self.conv_layer = nn.Conv2d(in_channels=3, out_channels=d_conv_channels, kernel_size=(11, 7), padding=(5, 3), stride=1)  # Convolution with 4 filters of size 11x7\n",
    "#         self.bypass_layer = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1, stride=1, bias=False)  # Convolution with 1 filter of size 1 to feed info directly to next layer.  Bias set to false so that non-holds are set to zero\n",
    "\n",
    "#         self.fc1 = nn.Sequential(nn.Linear((d_conv_channels + 3) * np.prod(board_size), 50), nn.Sigmoid())  # First fc layer with 5x18x11 neurons -> 50 neurons\n",
    "#         self.dropout = nn.Dropout(p=0.5)  # Dropout layer to reduce overtraining\n",
    "#         self.fc_grade = nn.Sequential(nn.Linear(50, num_grades), nn.Sigmoid())  # Second fc layer\n",
    "#         self.fc_rf = nn.Sequential(nn.Linear(50, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, problem):\n",
    "        #problem = (problem[:,0] + problem[:,1] + problem[:,2]).unsqueeze(1)\n",
    "\n",
    "        conv = self.conv_layer(problem)  # * problem\n",
    "        bypass = self.bypass_layer(problem)\n",
    "\n",
    "        conv = conv.view(conv.shape[0], -1)\n",
    "        bypass = bypass.view(bypass.shape[0], -1)\n",
    "        inter = t.cat((conv, bypass), 1)\n",
    "\n",
    "        inter = self.fc1(inter)\n",
    "        inter = self.dropout(inter)\n",
    "        grade = self.fc_grade(inter)\n",
    "        realfake = self.fc_rf(inter)\n",
    "        return [grade, realfake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18e2a87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_noise(name=None, size=batch_size, std=1, mean=0):\n",
    "    if name in ['rand', 'random']:\n",
    "        return t.rand((size, latent_dim))\n",
    "    return t.normal(mean=mean, std=std, size=(size, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbfa66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ != '__main__':\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a4b9c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ordinal_regression_loss(prediction, target):\n",
    "    return t.pow(nn.MSELoss(reduction='none')(prediction, target).sum(axis=1), 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3db836",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = Data()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = Adam(D.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71586bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.array(range(min(dataset.grades_numeric),\n",
    "                max(dataset.grades_numeric) + 2)) - 0.5\n",
    "hist, _ = np.histogram(dataset.grades_numeric, bins=bins)\n",
    "probabilities = hist/sum(hist)\n",
    "\n",
    "\n",
    "def generate_grades(size=batch_size):\n",
    "    grades = []\n",
    "    for _ in range(size):\n",
    "        vec = np.zeros(num_grades)\n",
    "        grade = np.random.choice(np.arange(1, num_grades + 1), p=probabilities)\n",
    "        vec[:grade] = 1\n",
    "        grades.append(vec)\n",
    "    return t.Tensor(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4189fd66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7170 3140 3831 2307 3439 3133 2001  847  926  625  237]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ret = sns.histplot(dataset.grades_numeric, bins=bins)\n",
    "plt.xlabel('Grade')\n",
    "plt.xticks(bins - 0.5)\n",
    "plt.title('Distribution of grades in dataset')\n",
    "\n",
    "hist, _ = np.histogram(dataset.grades_numeric, bins=bins)\n",
    "sns.scatterplot(x=bins[:-1] + 0.5, y=hist, color='red')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbc33de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_c/_nvtfkv15852mprwd5wgx0nw0000gn/T/ipykernel_2580/667383839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Generate sample problems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mgen_problems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_grades\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Loss measures generator's ability to fool the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_c/_nvtfkv15852mprwd5wgx0nw0000gn/T/ipykernel_2580/1529462740.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Run the convolutional blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_factor = 1\n",
    "n_batches = len(dataloader)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        # --------------------\n",
    "        # Train Discriminator\n",
    "        # --------------------\n",
    "\n",
    "        # Generate noise and labels to input into Generator\n",
    "        noise = generate_noise()\n",
    "        sampled_grades = generate_grades()\n",
    "\n",
    "        # Run the Generator to generate problems\n",
    "        gen_problems = G([noise, sampled_grades])\n",
    "\n",
    "        # Get some real problems and their associated grades\n",
    "        problems, grades = data\n",
    "        problems = problems.unsqueeze(1)\n",
    "\n",
    "        # Concatenate real problems and generated problems to run through the Discriminator\n",
    "        all_problems = t.cat((problems, gen_problems.detach()))\n",
    "        all_grades = t.cat((grades, sampled_grades))\n",
    "        truth_values = t.Tensor([1] * len(grades) + [0]\n",
    "                                * len(sampled_grades)).unsqueeze(1)\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Run the Discriminator to get its predictions\n",
    "        grade_prediction, realfake = D(all_problems)\n",
    "\n",
    "        # Compute the loss of the Discriminator\n",
    "        D_loss_grade = ordinal_regression_loss(\n",
    "            grade_prediction, all_grades)\n",
    "        D_loss_realfake = adversarial_loss(realfake, truth_values)\n",
    "\n",
    "        D_loss = D_loss_grade + loss_factor * D_loss_realfake\n",
    "\n",
    "        # Train the Discriminator\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ---------------\n",
    "        # Train Generator\n",
    "        # ---------------\n",
    "\n",
    "        # Generate noise and labels to input into Generator\n",
    "        # Train with twice as much data so that G and D train on the same number of problems\n",
    "        noise = generate_noise(size=2*batch_size)\n",
    "        sampled_grades = generate_grades(2*batch_size)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate sample problems\n",
    "        gen_problems = G([noise, sampled_grades])\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        grade_prediction, realfake = D(gen_problems)\n",
    "        G_loss_grade = ordinal_regression_loss(\n",
    "            grade_prediction, sampled_grades)\n",
    "        G_loss_realfake = adversarial_loss(\n",
    "            realfake, t.ones_like(realfake))\n",
    "\n",
    "        G_loss = G_loss_grade + loss_factor * G_loss_realfake\n",
    "\n",
    "    #         gen_problems_binary = t.where(gen_problems > 0.5, 1, 0)\n",
    "    #         num_holds = t.sum(gen_problems_binary, (2,3))\n",
    "    #         unique, counts = t.unique(num_holds, return_counts=True)\n",
    "    #         G_loss += counts[0].item()/10\n",
    "\n",
    "        # Train the Generator\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ----------------\n",
    "        # Log Progress\n",
    "        # ----------------\n",
    "\n",
    "        if (i+1) % (n_batches//8) == 0:\n",
    "            print(f'Epoch: {epoch+1}/{n_epochs}  Batch: {i+1}/{n_batches}  G loss: %.3f  D loss: %.3f' %\n",
    "                  (G_loss.item(), D_loss.item()))\n",
    "\n",
    "            #gen_problems = t.round(gen_problems)\n",
    "            gen_problems = t.where(gen_problems > 0.5, 1, 0)\n",
    "            index = np.random.choice(len(gen_problems))\n",
    "            print('Index: %d, Grade: %d\\n' % (index, t.sum(\n",
    "                sampled_grades[index]).item()), gen_problems[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b619835",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t.save(G, './generator.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
